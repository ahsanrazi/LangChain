{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcEtzYk1hoxZ5DsN0cGivS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsanrazi/LangChain/blob/main/03_ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyYqdhM6sCxO",
        "outputId": "ba030b18-6f20-44d9-bde1-1a0b39861938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaOtPEkRxDAE",
        "outputId": "0a9f2b87-0130-41a2-b697-312e2236d6a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\").strip()"
      ],
      "metadata": {
        "id": "FeWIKNhEtDVw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-exp\", api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "F2QtA6bkvN5Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First use the model directly\n",
        "\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model.invoke([HumanMessage(\"Hi I am Bob\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DofQGcmDvdO0",
        "outputId": "67ca606c-ff17-4291-cc24-92a8088b1062"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Bob! It's nice to meet you. How can I help you today?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-13a16c17-1b0d-430c-875a-acbf716a5bae-0', usage_metadata={'input_tokens': 5, 'output_tokens': 19, 'total_tokens': 24, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The model on its own does not have any concept of state.\n",
        "\n",
        "model.invoke([HumanMessage(\"What is my name?\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er8AhchHv2ZH",
        "outputId": "a9a9be0a-e6eb-40f9-f723-aaf56a33248d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I am an AI, and I don't have access to your personal information, so I don't know your name. You haven't told me what your name is. \\n\\nIf you'd like me to call you something, you can tell me what you'd prefer!\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-7b1ba28e-3ba2-47c5-bd02-098a32c59513-0', usage_metadata={'input_tokens': 6, 'output_tokens': 61, 'total_tokens': 67, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To get around this, we need to pass the entire conversation history into the model.\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
        "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNoB07dSwEM0",
        "outputId": "fb42ba8e-4a26-4ad2-c2f5-cf8fe8c45fd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Bob! You told me that at the beginning of our conversation. 😊', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-35be26f9-9587-4a6f-b1f4-e66b658d165d-0', usage_metadata={'input_tokens': 25, 'output_tokens': 18, 'total_tokens': 43, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Message persistence"
      ],
      "metadata": {
        "id": "gXPKztZjwutF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(state_schema=MessagesState)\n",
        "\n",
        "\n",
        "# Define the function that calls the model\n",
        "def call_model(state: MessagesState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "# Define the (single) node in the graph\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "# Add memory\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "IjXPFg3BwgxN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We now need to create a config that we pass into the runnable every time.\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "ihvbkTkXxTQ9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm Bob.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)"
      ],
      "metadata": {
        "id": "8ih46IM0zJV8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbIeOSKyzwOb",
        "outputId": "8f58bf3d-1dee-4ac6-e310-298f1c6d1e56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Bob, it's nice to meet you! I'm an AI, but you can think of me as a helpful assistant. What can I do for you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)"
      ],
      "metadata": {
        "id": "9oxG_gfj0zZE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isK1CuDM2iSE",
        "outputId": "dd822f00-e544-4c2a-fe52-4ca72111208f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob! You told me that at the beginning of our conversation. 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our chatbot now remembers things about us. If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"xyz987\"}}\n",
        "\n",
        "query = \"What's my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)"
      ],
      "metadata": {
        "id": "XQKI3L3q2FAc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi3BI-b92-vk",
        "outputId": "8101cd23-b9f0-4bb4-c18a-a47ed92e346d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "As a large language model, I don't have access to personal information about you. Therefore, I don't know your name. You haven't told me what it is.\n",
            "\n",
            "If you'd like to share it, I'd be happy to address you by your name! 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt templates"
      ],
      "metadata": {
        "id": "NrpMt1YH4ibs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Right now, all we've done is add a simple persistence layer around the model.\n",
        "# We can start to make the chatbot more complicated and personalized by adding in a prompt template.\n",
        "\n",
        "# Prompt Templates help to turn raw user information into a format that the LLM can work with.\n",
        "# To add in a system message, we will create a ChatPromptTemplate. We will utilize MessagesPlaceholder to pass all the messages in.\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\", \"You talk like a pirate. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "bjP2srip3Bnr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now update our application to incorporate this template\n",
        "\n",
        "workflow = StateGraph(MessagesState)\n",
        "\n",
        "\n",
        "def call_model(state: MessagesState):\n",
        "    prompt = prompt_template.invoke(state)\n",
        "    response = model.invoke(prompt)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "SC35MRhV5XTj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
        "query = \"Hi! I'm Jim.\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke({\"messages\": input_messages}, config)"
      ],
      "metadata": {
        "id": "P7ffkxTQ7ukT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N880i6I378Fr",
        "outputId": "e96209c5-c386-4ee4-cf52-e8014409f007"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Ahoy there, Jim! A fine name ye have, says I! What brings ye to me ol' ship today? Are ye lookin' for treasure, tales, or somethin' else entirely?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now make our prompt a little bit more complicated\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "nXmN-SKQ_SX6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence\n",
        "\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    language: str\n",
        "\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "def call_model(state: State):\n",
        "    prompt = prompt_template.invoke(state)\n",
        "    response = model.invoke(prompt)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "canPOamq_lAC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc456\"}}"
      ],
      "metadata": {
        "id": "pSEKKUOkAG7J"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Hi! I'm Bob.\"\n",
        "language = \"Spanish\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")"
      ],
      "metadata": {
        "id": "HW7CTBLOAAF6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utUnTbTkAFNO",
        "outputId": "ae549670-56e8-40d6-fc6c-55c6f5f911f2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "¡Hola Bob! Es un placer conocerte. ¿En qué puedo ayudarte hoy?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is my name?\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages},\n",
        "    config,\n",
        ")\n",
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1mm3ZJHAffi",
        "outputId": "2baa430a-fdcc-4df2-c35b-fff612894a95"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Tu nombre es Bob.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Managing Conversation History"
      ],
      "metadata": {
        "id": "hTiYDLgTA9oQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One important concept to understand when building chatbots is how to manage conversation history.\n",
        "# If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM.\n",
        "\n",
        "# Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.\n",
        "\n"
      ],
      "metadata": {
        "id": "2zVpXrTCAy5i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LangChain comes with a few built-in helpers for managing a list of messages.\n",
        "# In this case we'll use the trim_messages helper to reduce how many messages we're sending to the model.\n",
        "\n",
        "# The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message\n",
        "# and whether to allow partial messages.\n",
        "\n",
        "\n",
        "from langchain_core.messages import SystemMessage, trim_messages\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=65,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFNPJB_vxF7x",
        "outputId": "50bdaba9-3eb1-44b4-897b-60a1919654d8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now make our prompt a little bit more complicated\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "-ASC4DgX2jEG"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    language: str\n",
        "\n",
        "workflow = StateGraph(state_schema=State)\n",
        "\n",
        "\n",
        "def call_model(state: State):\n",
        "    # trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
        "    trimmed_messages = trimmer.invoke([m for m in state[\"messages\"] if not isinstance(m, SystemMessage)])\n",
        "    prompt = prompt_template.invoke(\n",
        "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
        "    )\n",
        "    response = model.invoke(prompt)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "workflow.add_edge(START, \"model\")\n",
        "workflow.add_node(\"model\", call_model)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "Kh4VmUpF0lQO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc879\"}}\n",
        "query = \"What is my name?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")"
      ],
      "metadata": {
        "id": "TLn6G9IM0WIP"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnwXfkzL1rT2",
        "outputId": "048b7f2b-d82d-4f84-d5a7-e53cb8646a1d"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Your name is Bob. You told me that at the beginning. 😊\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
        "query = \"What math problem did I ask?\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = messages + [HumanMessage(query)]\n",
        "output = app.invoke(\n",
        "    {\"messages\": input_messages, \"language\": language},\n",
        "    config,\n",
        ")"
      ],
      "metadata": {
        "id": "gF-_Tmbv5laU"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihDaIRP172ek",
        "outputId": "03c55350-89b6-4d40-a236-08c1316cf1ba"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "You asked \"what's 2 + 2\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streaming"
      ],
      "metadata": {
        "id": "IvQ_1pnh8uzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLMs can sometimes take a while to respond, and so in order to improve the user experience\n",
        "# one thing that most applications do is stream back each token as it is generated."
      ],
      "metadata": {
        "id": "oS-5Trr980Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
        "query = \"Hi I'm Todd, please tell me a joke.\"\n",
        "language = \"English\"\n",
        "\n",
        "input_messages = [HumanMessage(query)]\n",
        "for chunk, metadata in app.stream({\"messages\": input_messages, \"language\": language}, config, stream_mode=\"messages\",):\n",
        "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
        "        print(chunk.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXkDOY-i7360",
        "outputId": "066aae48-19c0-4f83-e6a7-013ccdd40ddd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay| Todd, here's another one for you:\n",
            "\n",
            "Why did the bicycle fall| over? \n",
            "\n",
            "Because it was two tired!|"
          ]
        }
      ]
    }
  ]
}