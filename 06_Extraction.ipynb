{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVsy8OKxtj/XpcqLlqeGaj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsanrazi/LangChain/blob/main/06_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-core\n",
        "!pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "id": "f1DRMFmX4QBu"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY').strip()"
      ],
      "metadata": {
        "id": "MhQAJrcG4XH1"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraction"
      ],
      "metadata": {
        "id": "Xr5-CacM3Lb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "wUoMb9453BEH"
      },
      "outputs": [],
      "source": [
        "# Extract structured data from text and other unstructured media using chat models and few-shot examples."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Schema"
      ],
      "metadata": {
        "id": "du1u1VTS3kNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we need to describe what information we want to extract from the text."
      ],
      "metadata": {
        "id": "12dFFkho3j7t"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use Pydantic to define an example schema to extract personal information.\n",
        "\n",
        "from typing import Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    # ^ Doc-string for the entity Person.\n",
        "    # This doc-string is sent to the LLM as the description of the schema Person, and it can help to improve extraction results.\n",
        "\n",
        "    # Note that:\n",
        "    # 1. Each field is an `optional` -- this allows the model to decline to extract it!\n",
        "    # 2. Each field has a `description` -- this description is used by the LLM.\n",
        "    # Having a good description can help improve extraction results.\n",
        "\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(default=None, description=\"Hair color of the person\")\n",
        "    height_in_meters: Optional[str] = Field(default=None, description=\"Height in meters\")"
      ],
      "metadata": {
        "id": "792wCeVK3d2N"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are two best practices when defining schema:\n",
        "\n",
        "# Document the attributes and the schema itself: This information is sent to the LLM and is used to improve the quality of information extraction.\n",
        "# Do not force the LLM to make up information! Above we used Optional for the attributes allowing the LLM to output None if it doesn't know the answer."
      ],
      "metadata": {
        "id": "t2sXAEnX4MKW"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Extractor"
      ],
      "metadata": {
        "id": "rmuuUVlw4nf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create an information extractor using the schema we defined above.\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Define a custom prompt to provide instructions and any additional context.\n",
        "# 1) You can add examples into the prompt template to improve extraction quality\n",
        "# 2) Introduce additional parameters to take context into account (e.g., include metadata about the document from which the text was extracted.)\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm. Only extract relevant information from the text. If you do not know the value of an attribute asked to extract, return null for the attribute's value.\",\n",
        "        ),\n",
        "        # Please see the how-to about improving performance with MessagesPlaceholder('examples'),\n",
        "        (\"human\", \"{text}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "kwyh36bM4nP0"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-exp\", api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "6qYEazOY6FdW"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Person)"
      ],
      "metadata": {
        "id": "ea_Ap0hM6TsF"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Raza is 6 feet tall and has a Red hair.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "response = structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "IBgXARzg6hrV"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIZMzyp__hgB",
        "outputId": "f2a996ef-23c4-4115-cd9e-c42e68db8bc0"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Person(name='Raza', hair_color=None, height_in_meters='1.8288')"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple Entities"
      ],
      "metadata": {
        "id": "dYaB4nO__dcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In most cases, we should be extracting a list of entities rather than a single entity.\n",
        "# This can be easily achieved using pydantic by nesting models inside one another.\n",
        "\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    name: Optional[str] = Field(default=None, description=\"The name of the person\")\n",
        "    hair_color: Optional[str] = Field(default=None, description=\"Hair color of the person\")\n",
        "    height_in_meters: Optional[str] = Field(default=None, description=\"Height in meters\")\n",
        "\n",
        "\n",
        "class Data(BaseModel):\n",
        "    \"\"\"Extracted data about people.\"\"\"\n",
        "\n",
        "    # Creates a model so that we can extract multiple entities.\n",
        "    people: List[Person]"
      ],
      "metadata": {
        "id": "aZH7UqQ87IOk"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = llm.with_structured_output(schema=Data)"
      ],
      "metadata": {
        "id": "avpxGy0W_tjR"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My name is Jeff, my hair is black and I am 6 feet tall. Anna has the same color hair as me.\"\n",
        "prompt = prompt_template.invoke({\"text\": text})\n",
        "response = structured_llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "zmm9JwDeAMXq"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3edgtx7AgAp",
        "outputId": "33d2ca45-d0d8-4e11-92dc-54c46ae74364"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(people=[Person(name='Jeff', hair_color=None, height_in_meters=None), Person(name='Anna', hair_color=None, height_in_meters=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Structured output often uses tool calling under-the-hood.\n",
        "# This typically involves the generation of AI messages containing tool calls, as well as tool messages containing the results of tool calls."
      ],
      "metadata": {
        "id": "h-DsUZk9Ah-j"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MessagesPlaceholder"
      ],
      "metadata": {
        "id": "d8bWVGp4CakY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "prompt = MessagesPlaceholder(\"history\")\n",
        "prompt.format_messages() # raises KeyError\n",
        "\n",
        "prompt = MessagesPlaceholder(\"history\", optional=True)\n",
        "prompt.format_messages() # returns empty list []\n",
        "\n",
        "prompt.format_messages(\n",
        "    history=[\n",
        "        (\"system\", \"You are an AI assistant.\"),\n",
        "        (\"human\", \"Hello!\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# -> [\n",
        "#     SystemMessage(content=\"You are an AI assistant.\"),\n",
        "#     HumanMessage(content=\"Hello!\"),\n",
        "# ]"
      ],
      "metadata": {
        "id": "XfSf8ZZ4CRLx"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a prompt with chat history:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant.\"),\n",
        "        MessagesPlaceholder(\"history\"),\n",
        "        (\"human\", \"{question}\")\n",
        "    ]\n",
        ")\n",
        "prompt.invoke(\n",
        "   {\n",
        "       \"history\": [(\"human\", \"what's 5 + 2\"), (\"ai\", \"5 + 2 is 7\")],\n",
        "       \"question\": \"now multiply that by 4\"\n",
        "   }\n",
        ")\n",
        "# -> ChatPromptValue(messages=[\n",
        "#     SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "#     HumanMessage(content=\"what's 5 + 2\"),\n",
        "#     AIMessage(content=\"5 + 2 is 7\"),\n",
        "#     HumanMessage(content=\"now multiply that by 4\"),\n",
        "# ])"
      ],
      "metadata": {
        "id": "2ZKxICsSCrMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limiting the number of messages:\n",
        "\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "prompt = MessagesPlaceholder(\"history\", n_messages=1)\n",
        "\n",
        "prompt.format_messages(\n",
        "    history=[\n",
        "        (\"system\", \"You are an AI assistant.\"),\n",
        "        (\"human\", \"Hello!\"),\n",
        "    ]\n",
        ")\n",
        "# -> [\n",
        "#     HumanMessage(content=\"Hello!\"),\n",
        "# ]"
      ],
      "metadata": {
        "id": "guhomwltDE4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use a Parsing Approach"
      ],
      "metadata": {
        "id": "TMPtMpNYOpf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a prompt based approach to extract with models that do not support tool/function calling."
      ],
      "metadata": {
        "id": "OhhQ_rFaOfZu"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tool calling features are not required for generating structured output from LLMs.\n",
        "# LLMs that are able to follow prompt instructions well can be tasked with outputting information in a given format.\n",
        "\n",
        "# This approach relies on designing good prompts and then parsing the output of the LLMs to make them extract information well.\n",
        "\n",
        "# To extract data without tool-calling features:\n",
        "# Instruct the LLM to generate text following an expected format (e.g., JSON with a certain schema);\n",
        "# Use output parsers to structure the model response into a desired Python object."
      ],
      "metadata": {
        "id": "xVOK0zDjO2Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PydanticOutputParser"
      ],
      "metadata": {
        "id": "f3Huaz5CPaGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Optional\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field, validator\n",
        "\n",
        "\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "\n",
        "    name: str = Field(..., description=\"The name of the person\")\n",
        "    height_in_meters: float = Field(..., description=\"The height of the person expressed in meters.\")  # ... means this field is required\n",
        "\n",
        "\n",
        "class People(BaseModel):\n",
        "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
        "\n",
        "    people: List[Person]"
      ],
      "metadata": {
        "id": "gyGpZ9x9PbG0"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a parser\n",
        "parser = PydanticOutputParser(pydantic_object=People)\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"),\n",
        "        (\"human\", \"{query}\"),\n",
        "    ]\n",
        ").partial(format_instructions=parser.get_format_instructions())"
      ],
      "metadata": {
        "id": "Zdcxc7wWQBkd"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look at what information is sent to the model\n",
        "\n",
        "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
        "\n",
        "print(prompt.format_prompt(query=query).to_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh-avtNBTmzE",
        "outputId": "11c8718b-bda6-45bb-a921-2fd66a033f2c"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Answer the user query. Wrap the output in `json` tags\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"$defs\": {\"Person\": {\"description\": \"Information about a person.\", \"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"height_in_meters\": {\"description\": \"The height of the person expressed in meters.\", \"title\": \"Height In Meters\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"], \"title\": \"Person\", \"type\": \"object\"}}, \"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"items\": {\"$ref\": \"#/$defs/Person\"}, \"title\": \"People\", \"type\": \"array\"}}, \"required\": [\"people\"]}\n",
            "```\n",
            "Human: Anna is 23 years old and she is 6 feet tall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we simply chain together the prompt, model and output parser\n",
        "\n",
        "chain = prompt | llm | parser\n",
        "chain.invoke({\"query\": query})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2PdBkdnUTnU",
        "outputId": "bc33efd6-d0ff-4232-b86e-5d127df50733"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "People(people=[Person(name='Anna', height_in_meters=1.8288)])"
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    }
  ]
}