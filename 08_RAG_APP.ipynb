{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/90tdaB0JDoHuBNKrZuhd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsanrazi/LangChain/blob/main/08_RAG_APP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Augmented Generation (RAG) App"
      ],
      "metadata": {
        "id": "gulvonQTM78d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('GEMINI_API_KEY').strip()"
      ],
      "metadata": {
        "id": "icF2AVQyRuFj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_6o0Y5GGiXz",
        "outputId": "b0f4203a-da9e-45dc-f2aa-48b4fc9d3831"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/427.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langgraph\n",
        "!pip install -qU langchain-text-splitters\n",
        "!pip install -qU langchain-community\n",
        "!pip install -qU langchain-google-genai\n",
        "!pip install -qU langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots.\n",
        "# These are applications that can answer questions about specific source information.\n",
        "# These applications use a technique known as Retrieval Augmented Generation, or RAG."
      ],
      "metadata": {
        "id": "maBikYiYNAoE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ],
      "metadata": {
        "id": "Y6NOKKGcNquj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A typical RAG application has two main components\n",
        "\n",
        "# A typical RAG application has two main components:\n",
        "# Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
        "# Retrieval and generation: The actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index,\n",
        "# then passes that to the model."
      ],
      "metadata": {
        "id": "EuxljaOjNrUx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing"
      ],
      "metadata": {
        "id": "BFwKBQ5_QCEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load: First we need to load our data. This is done with Document Loaders.\n",
        "\n",
        "# Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model,\n",
        "# as large chunks are harder to search over and won't fit in a model's finite context window.\n",
        "\n",
        "# Embed-Store: We need somewhere to store and index our splits, so that they can be searched over later.\n",
        "# This is often done using a VectorStore and Embeddings model."
      ],
      "metadata": {
        "id": "l03QyPuAQBUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval and generation"
      ],
      "metadata": {
        "id": "HvihUIsmQkr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
        "# Generate: A ChatModel / LLM produces an answer using a prompt that includes both the question with the retrieved data."
      ],
      "metadata": {
        "id": "n5r2kNV3QlYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Model"
      ],
      "metadata": {
        "id": "UZSCFC2hShgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "llm = GoogleGenerativeAIEmbeddings(model = \"gemini-2.0-flash-exp\", api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "_hetk3l4SjQn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Model"
      ],
      "metadata": {
        "id": "C4oQp80IS8HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key = gemini_api_key)"
      ],
      "metadata": {
        "id": "agNyDlI0S5PQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectore Store"
      ],
      "metadata": {
        "id": "4MWAdNdjYQXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone\n",
        "\n",
        "index_name = \"langchain\"\n",
        "namespace = \"RAG\"\n",
        "\n",
        "pc = Pinecone(api_key= userdata.get('PINECONE_API'))\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "vector_store = PineconeVectorStore(embedding=embeddings, index=index, namespace=namespace)"
      ],
      "metadata": {
        "id": "4kY60LV6YcNo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Indexing"
      ],
      "metadata": {
        "id": "ip6oeNbaVUns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading documents"
      ],
      "metadata": {
        "id": "NaIyXJwZVZGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects\n",
        "# that load in data from a source and return a list of Document objects.\n",
        "\n",
        "# In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text."
      ],
      "metadata": {
        "id": "NDsTH_u4Vaqp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs_kwargs={\"parse_only\": bs4_strainer})\n",
        "docs = loader.load()\n",
        "\n",
        "assert len(docs) == 1\n",
        "print(f\"Total characters: {len(docs[0].page_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41zU92GoWAaB",
        "outputId": "4579131a-d114-4532-ad54-c23f6d87ea4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 43130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB3ADuXpWnku",
        "outputId": "a8296128-2183-447c-c24d-b2b8be9c91f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting documents"
      ],
      "metadata": {
        "id": "R8J0c5r7Xo24"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SbzpcwlTXdhH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}